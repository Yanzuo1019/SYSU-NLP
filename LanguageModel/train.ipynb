{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 1024\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "data_path = \"data/result.utf8\"\n",
    "\n",
    "word2id = {}\n",
    "id2word = []\n",
    "word_num = 0\n",
    "\n",
    "word2id[\"<PAD>\"] = word_num\n",
    "id2word.append(\"<PAD>\")\n",
    "word_num += 1\n",
    "\n",
    "word2id[\"<EOS>\"] = word_num\n",
    "id2word.append(\"<EOS>\")\n",
    "word_num += 1\n",
    "\n",
    "sentences = []\n",
    "seq_lens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0., bidirectional=False):\n",
    "        super(MaskedLSTM, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, bias=bias,\n",
    "             batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input_tensor, seq_lens):\n",
    "        total_length = input_tensor.size(1) if self.batch_first else input_tensor.size(0)\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(input_tensor, seq_lens, batch_first=self.batch_first, enforce_sorted=False)\n",
    "        y_lstm, hidden = self.lstm(x_packed)\n",
    "        y_padded, length = nn.utils.rnn.pad_packed_sequence(y_lstm, batch_first=self.batch_first, total_length=total_length)\n",
    "        return y_padded, hidden\n",
    "\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.word_embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = MaskedLSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, sentences, seq_lens):\n",
    "        batch_size, time_step = sentences.shape\n",
    "        embedding = self.word_embed(sentences)\n",
    "        out, _ = self.lstm(embedding, seq_lens)\n",
    "        out = self.linear(out.view(batch_size * time_step, -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentences, max_len):\n",
    "    batch = []\n",
    "    index = []\n",
    "    for i, sen in enumerate(sentences):\n",
    "        tensor = sen.copy()\n",
    "        tensor.extend([word2id[\"<PAD>\"]] * (max_len - len(sen)))\n",
    "        tensor = torch.LongTensor(tensor)\n",
    "        batch.append(tensor)\n",
    "        index.extend([j for j in range(i * max_len, i * max_len + len(sen))])\n",
    "    return torch.stack(batch), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, \"r\", encoding=\"utf8\") as data:\n",
    "    for line in data:\n",
    "        line_split = line.strip().split()\n",
    "\n",
    "        sen = []\n",
    "        for word in line_split:\n",
    "            if word not in word2id:\n",
    "                word2id[word] = word_num\n",
    "                id2word.append(word)\n",
    "                word_num += 1\n",
    "            sen.append(word2id[word])\n",
    "        \n",
    "        sen.append(word2id[\"<EOS>\"])\n",
    "        sentences.append(sen)\n",
    "        seq_lens.append(len(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)\n",
    "model = RNNLM(vocab_size, EMBEDDING_SIZE, HIDDEN_SIZE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=1, verbose=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}